{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import bayesopt\n",
    "\n",
    "import Models.Reversible_Classifier as MC\n",
    "import Models.Net as Net\n",
    "\n",
    "import Data_Handler.Classifier_data as CD\n",
    "\n",
    "Classifier_data = CD.Classifier_data\n",
    "CNet = MC.Classifier\n",
    "\n",
    "# Surpress print statements while training\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = None\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Data'\n",
    "g = pd.read_csv(os.path.join(data_dir, \n",
    "    'LupusGeneExpressionCompendium_AllGfeatures.csv'))\n",
    "l = pd.read_csv(os.path.join(data_dir,\n",
    "    'PatientDx_Labels.csv'))\n",
    "\n",
    "df = pd.DataFrame({'trt':[0,1,2], 'sle_class':['Healthy_No_Treatment','SLE_No_Treatment', 'SLE_Treatment']})\n",
    "#Merge the dataframe created with the labels dataset\n",
    "df2 = pd.merge(l, df, how='outer', on='sle_class', left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "\n",
    "#Merge all datasets together\n",
    "#research ready dataset\n",
    "df3 = pd.merge(g, df2, how='inner', on='patid', left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "\n",
    "#split the dataset into data and labels\n",
    "\n",
    "df_data = df3.iloc[:,1:-2]\n",
    "df_labels = df3.loc[:, 'trt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNet = MC.Classifier\n",
    "Classifier_data = CD.Classifier_data\n",
    "Lupus_data = Classifier_data(100, df_data.values, df_labels.values)\n",
    "N_outputs = Lupus_data.Num_of_classes\n",
    "N_inputs = Lupus_data.Num_of_features\n",
    "config = CNet.get_default_configs(N_inputs, N_outputs)\n",
    "# config['hyperparameter']['lr'] = 3e-5\n",
    "cnet = CNet(config, Lupus_data, [0.6, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "def BO_loss_functions(N):\n",
    "    def classifierLoss(i):\n",
    "        inputlist = i.tolist()\n",
    "        tunables = ['lr','beta1', 'beta2', 'epsilon', 'max_epochs']\n",
    "        hp = dict(zip(tunables, inputlist))\n",
    "    \n",
    "        #Casting to an int to allow max epochs to be a valid value\n",
    "        hp['max_epochs'] = int(hp['max_epochs'])\n",
    "        total_loss = 0\n",
    "    \n",
    "        #Number of samples to average\n",
    "        N = N\n",
    "        print(N)\n",
    "    \n",
    "        for n in range(0, N):\n",
    "            with HiddenPrints():\n",
    "                cnet.train(save_every=0, eval_every = 10000, \n",
    "                       hyperparameters = hp)\n",
    "                total_loss += cnet.val_loss\n",
    "        return total_loss/N\n",
    "    \n",
    "    def generatorLoss(i):\n",
    "        inputlist = i.tolist()\n",
    "        tunables = ['lr','beta1', 'beta2', 'epsilon', 'max_epochs']\n",
    "        hp = dict(zip(tunables, inputlist))\n",
    "    \n",
    "        #Casting to an int to allow max epochs to be a valid value\n",
    "        hp['max_epochs'] = int(hp['max_epochs'])\n",
    "        total_loss = 0\n",
    "    \n",
    "        #Number of samples to average\n",
    "        N = N\n",
    "        print(N)\n",
    "    \n",
    "        for n in range(0, N):\n",
    "            with HiddenPrints():\n",
    "                cnet.train_generator(save_every=0, eval_every = 10000, \n",
    "                       hyperparameters = hp)\n",
    "                total_loss += cnet.val_loss\n",
    "        return total_loss/N\n",
    "    \n",
    "    return classifierLoss, generatorLoss\n",
    "\n",
    "classifierLoss, generatorLoss = BO_loss_functions(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting noise\n",
    "val_losses = []\n",
    "print('Finding noise with %d samples' % (5 * N))\n",
    "with HiddenPrints():\n",
    "    for i in range (0, 5 * N):\n",
    "        cnet.train(save_every=0, eval_every=1000, \n",
    "                   hyperparameters={'lr':3e-5, 'max_epochs': 10})\n",
    "        val_losses.append(cnet.val_loss)\n",
    "\n",
    "noise = np.std(val_losses)\n",
    "print('Classifier Validation Loss Noise: %02.4f' % noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lb = np.array([1e-10, 1e-10, 1e-10, 1e-15, 10])\n",
    "ub = np.array([1e-2, 0.99, 0.99, 1, 100])\n",
    "params = []\n",
    "params['n_iterations'] = 75\n",
    "params['noise'] = 2 * noise/sqrt(N) #Conservative noise estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_loss, C_hp, error = bayesopt.optimize(classifierLoss,\n",
    "                                       5, lb, ub, params)\n",
    "inputlist = C_hp.tolist()\n",
    "tunables = ['lr','beta1', 'beta2', 'epsilon', 'max_epochs']\n",
    "hp = dict(zip(tunables, inputlist))\n",
    "hp['max_epochs'] = int(hp['max_epochs'])\n",
    "\n",
    "cnet.train(save_every = 5, eval_every = 5,\n",
    "          hyperparamters = hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting noise\n",
    "val_losses = []\n",
    "print('Finding noise with %d samples' % (5 * N))\n",
    "with HiddenPrints():\n",
    "    for i in range (0, 5 * N):\n",
    "        cnet.train_generator(save_every=0, eval_every=1000, \n",
    "                   hyperparameters={'lr':3e-5, 'max_epochs': 10})\n",
    "        val_losses.append(cnet.val_loss)\n",
    "\n",
    "noise = np.std(val_losses) \n",
    "print('Classifier Validation Loss Noise: %02.4f' % noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_loss, G_hp, error = bayesopt.optimize(generatorLoss,\n",
    "                                       5, lb, ub, params)\n",
    "\n",
    "inputlist = C_hp.tolist()\n",
    "tunables = ['lr','beta1', 'beta2', 'epsilon', 'max_epochs']\n",
    "hp = dict(zip(tunables, inputlist))\n",
    "hp['max_epochs'] = int(hp['max_epochs'])\n",
    "\n",
    "cnet.train_generator(save_every = 5, eval_every = 5,\n",
    "          hyperparamters = hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnet.save(1000)\n",
    "cnet.sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
